"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1516],{31400:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>d});var s=t(74848),a=t(28453);const i={description:"Learn about the integrated raft storage in OpenBao."},o="Integrated storage",r={id:"internals/integrated-storage",title:"Integrated storage",description:"Learn about the integrated raft storage in OpenBao.",source:"@site/content/docs/internals/integrated-storage.mdx",sourceDirName:"internals",slug:"/internals/integrated-storage",permalink:"/docs/internals/integrated-storage",draft:!1,unlisted:!1,editUrl:"https://github.com/openbao/openbao/tree/main/website/content/docs/internals/integrated-storage.mdx",tags:[],version:"current",frontMatter:{description:"Learn about the integrated raft storage in OpenBao."},sidebar:"docs",previous:{title:"High availability",permalink:"/docs/internals/high-availability"},next:{title:"Security model",permalink:"/docs/internals/security"}},l={},d=[{value:"Consensus protocol",id:"consensus-protocol",level:2},{value:"Raft protocol overview",id:"raft-protocol-overview",level:3},{value:"Terminology",id:"terminology",level:4},{value:"Node states",id:"node-states",level:4},{value:"Writing logs",id:"writing-logs",level:4},{value:"Compacting logs",id:"compacting-logs",level:4},{value:"Quorum",id:"quorum",level:4},{value:"Performance",id:"performance",level:4},{value:"Raft in OpenBao",id:"raft-in-openbao",level:3},{value:"Quorum management",id:"quorum-management",level:3},{value:"Autopilot",id:"autopilot",level:4},{value:"Without autopilot",id:"without-autopilot",level:4},{value:"Deployment table",id:"deployment-table",level:3},{value:"Minimums &amp; scaling",id:"minimums--scaling",level:3}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"integrated-storage",children:"Integrated storage"}),"\n",(0,s.jsx)(n.p,{children:"OpenBao supports several storage options for the durable storage of OpenBao's\ninformation. Each backend offers pros, cons, advantages, and trade-offs. For\nexample, some backends support high availability while others provide a more\nrobust backup and restoration process."}),"\n",(0,s.jsx)(n.p,{children:"An Integrated Storage option is offered. This storage backend\ndoes not rely on any third party systems; it implements high availability and\nprovides backup/restore workflows."}),"\n",(0,s.jsx)(n.h2,{id:"consensus-protocol",children:"Consensus protocol"}),"\n",(0,s.jsxs)(n.p,{children:["OpenBao's Integrated Storage uses a ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Consensus_(computer_science)",children:"consensus\nprotocol"})," to provide\n",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/CAP_theorem",children:"Consistency"})," (as defined by CAP).\nThe consensus protocol is based on ",(0,s.jsx)(n.a,{href:"https://raft.github.io/raft.pdf",children:'"Raft: In search of an Understandable\nConsensus Algorithm"'}),". For a visual explanation\nof Raft, see ",(0,s.jsx)(n.a,{href:"http://thesecretlivesofdata.com/raft",children:"The Secret Lives of Data"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"raft-protocol-overview",children:"Raft protocol overview"}),"\n",(0,s.jsxs)(n.p,{children:["Raft is a consensus algorithm that is based on\n",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Paxos_%28computer_science%29",children:"Paxos"}),". Compared\nto Paxos, Raft is designed to have fewer states and a simpler, more\nunderstandable algorithm."]}),"\n",(0,s.jsxs)(n.p,{children:["The Raft protocol will not be fully covered here. However, a high level description is\nprovided to help you build a mental model. Refer to the\ncomplete specification that's described in ",(0,s.jsx)(n.a,{href:"https://raft.github.io/raft.pdf",children:"this paper"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"terminology",children:"Terminology"}),"\n",(0,s.jsx)(n.p,{children:"There are a few key terms to know when discussing Raft:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Leader"})," - At any given time, the peer set elects a single node to be the leader.\nThe leader is responsible for ingesting new log entries, replicating to followers,\nand managing when an entry is committed. The leader node is also the active OpenBao node and followers are standby nodes. Refer to the ",(0,s.jsx)(n.a,{href:"/docs/internals/high-availability#design-overview",children:"High Availability"})," document for more information."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Log"})," - An ordered sequence of entries (replicated log) to keep track of any cluster changes. The leader is responsible for ",(0,s.jsx)(n.em,{children:"log replication"}),". When new data is written, for example, a new event creates a log entry. The leader then sends the new log entry to its followers. Any inconsistency within the replicated log entries will indicate an issue."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"FSM"})," - ",(0,s.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Finite-state_machine",children:"Finite State Machine"}),".\nA collection of finite states with transitions between them. As new logs\nare applied, the FSM is allowed to transition between states. Application of the\nsame sequence of logs must result in the same state, meaning behavior must be deterministic."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Peer set"})," - The set of all members participating in log replication. All server nodes are in the peer set of the local cluster."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Quorum"})," - A majority of members from a peer set: for a set of size ",(0,s.jsx)(n.code,{children:"n"}),",\nquorum requires at least ",(0,s.jsx)(n.code,{children:"(n+1)/2"})," members. For example, if there are 5 members\nin the peer set, we would need 3 nodes to form a quorum. If a quorum of nodes is\nunavailable for any reason, the cluster becomes ",(0,s.jsx)(n.em,{children:"unavailable"})," and no new logs\ncan be committed."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Committed Entry"})," - An entry is considered ",(0,s.jsx)(n.em,{children:"committed"})," when it is durably stored\non a quorum of nodes. An entry is applied once its committed."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"node-states",children:"Node states"}),"\n",(0,s.jsx)(n.p,{children:"Raft nodes are always in one of three states: follower, candidate, or leader. All\nnodes initially start out as a follower. In this state, nodes can accept log entries\nfrom a leader and cast votes. If no entries are received for a period of time, nodes\nwill self-promote to the candidate state. In the candidate state, nodes request votes from their peers. If a candidate receives a quorum of votes, then it is promoted to a leader. The leader must accept new log entries and replicate to all the other followers."}),"\n",(0,s.jsx)(n.h4,{id:"writing-logs",children:"Writing logs"}),"\n",(0,s.jsxs)(n.p,{children:["Once a cluster has a leader, it is able to accept new log entries. A client can\nrequest that a leader append a new log entry (from Raft's perspective, a log entry\nis an opaque binary blob). The leader then writes the entry to durable storage and\nattempts to replicate to a quorum of followers. Once the log entry is considered\n",(0,s.jsx)(n.em,{children:"committed"}),", it can be ",(0,s.jsx)(n.em,{children:"applied"})," to a finite state machine. The finite state machine\nis application specific; in OpenBao's case, we use\n",(0,s.jsx)(n.a,{href:"https://github.com/etcd-io/bbolt",children:"BoltDB"})," to maintain a cluster state. OpenBao's writes\nare blocked until they are ",(0,s.jsx)(n.em,{children:"committed"})," and ",(0,s.jsx)(n.em,{children:"applied"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"compacting-logs",children:"Compacting logs"}),"\n",(0,s.jsx)(n.p,{children:"It would be undesirable to allow a replicated log to grow in an unbounded\nfashion. Raft provides a mechanism by which the current state is saved to\nsnapshots and its related logs are compacted. Because of the FSM abstraction,\nrestoring the state of the FSM must result in the same state as a replay of old\nlogs. This allows Raft to capture the FSM state at a point in time and then remove\nall the logs that were used to reach that state. This is performed automatically\nwithout user intervention and prevents unbounded disk usage while also minimizing\nthe time spent replaying logs. One of the advantages of using BoltDB is that it\nallows OpenBao's snapshots to be very light weight. Since OpenBao's data is already\npersisted to disk in BoltDB, the snapshot process just needs to truncate the raft logs."}),"\n",(0,s.jsx)(n.h4,{id:"quorum",children:"Quorum"}),"\n",(0,s.jsxs)(n.p,{children:["Consensus is fault-tolerant while a cluster has quorum.\nIf a quorum of nodes is unavailable, it is impossible to process log entries or reason\nabout peer membership. For example, suppose there are only 2 peers: A and B. The quorum\nsize is also 2, meaning both nodes must agree to commit a log entry. If either A or B\nfails, it is now impossible to reach quorum. This means the cluster is unable to add\nor remove a node or to commit any additional log entries. This results in\n",(0,s.jsx)(n.em,{children:"unavailability"}),". At this point, manual intervention is required to remove\neither A or B and restart the remaining node in bootstrap mode."]}),"\n",(0,s.jsxs)(n.p,{children:["A Raft cluster of 3 nodes can tolerate a single node failure while a cluster of\n5 can tolerate 2 node failures. The recommended OpenBao production deployment is\nto run 5 OpenBao servers per cluster. See the ",(0,s.jsx)(n.a,{href:"#minimums-scaling",children:"Minimum &\nScaling"})," and ",(0,s.jsx)(n.a,{href:"#deployment-table",children:"Deployment Table"})," to learn\nmore about the failure tolerance using Integrated Storage."]}),"\n",(0,s.jsx)(n.h4,{id:"performance",children:"Performance"}),"\n",(0,s.jsx)(n.p,{children:"In terms of performance, Raft is comparable to Paxos. Assuming stable leadership,\ncommitting a log entry requires a single round trip to half of the cluster.\nThus, performance is bound by disk I/O and network latency."}),"\n",(0,s.jsx)(n.h3,{id:"raft-in-openbao",children:"Raft in OpenBao"}),"\n",(0,s.jsxs)(n.p,{children:["When getting started, a single OpenBao server is\n",(0,s.jsx)(n.a,{href:"/docs/commands/operator/init/#operator-init",children:"initialized"}),". At this point, the\ncluster is of size 1, which allows the node to self-elect as a leader. Once a\nleader is elected, other servers can be added to the peer set in a way that\npreserves consistency and safety."]}),"\n",(0,s.jsx)(n.p,{children:"The join process is how new nodes are added to the OpenBao cluster; it uses an\nencrypted challenge/answer workflow. To accomplish this, all nodes in a single\nRaft cluster must share the same seal configuration. If using an Auto Unseal, the\njoin process can use the configured seal to automatically decrypt the challenge\nand respond with the answer. If using a Shamir seal, the unseal keys must be\nprovided to the node attempting to join the cluster before it can decrypt the\nchallenge and respond with the decrypted answer."}),"\n",(0,s.jsx)(n.p,{children:"Since all servers participate as part of the peer set, they all know the current\nleader. When an API request arrives at a non-leader server, the request is\nforwarded to the leader."}),"\n",(0,s.jsx)(n.p,{children:"Similar to other storage backends, data that is written to the Raft log and FSM\nwill be encrypted by OpenBao's barrier."}),"\n",(0,s.jsx)(n.h3,{id:"quorum-management",children:"Quorum management"}),"\n",(0,s.jsx)(n.h4,{id:"autopilot",children:"Autopilot"}),"\n",(0,s.jsxs)(n.p,{children:["An ",(0,s.jsx)(n.a,{href:"/docs/concepts/integrated-storage/autopilot",children:"Autopilot feature"}),"\nis available since 1.7.x & later versions that include configurable parameters\nfor when a node is treated as healthy before it's considered an eligible voter in the\nquorum list. Other features which may be enabled include the ability to remove nodes\nconsidered as dead from the quorum list after a certain period."]}),"\n",(0,s.jsx)(n.p,{children:"Autopilot is enabled by default and the default configuration values\nshould work well for most OpenBao deployments, but they can be changed if needed."}),"\n",(0,s.jsx)(n.p,{children:"Autopilot includes stabilization logic for nodes joining the cluster.\nRecently joined nodes are\naccepted as non-voter initially until they are in sync with matching Raft index\nand only after a stability thresholds are they then full voting members.\nSetting the stability threshold too low can result in cluster instability as nodes will be\ncounted as voters before they are capable of voting."}),"\n",(0,s.jsxs)(n.p,{children:["A dead server cleanup capability is available. With this feature\nenabled, unhealthy nodes are automatically removed from the Raft cluster without\nmanual operator intervention. This is enabled via the ",(0,s.jsx)(n.a,{href:"/api-docs/system/storage/raftautopilot",children:"Autopilot API"}),".\nIf you wish to decommission a node manually, this can be done with the\n",(0,s.jsx)(n.code,{children:"remove peer"})," ",(0,s.jsx)(n.a,{href:"/docs/commands/operator/raft#remove-peer",children:"command"}),"."]}),"\n",(0,s.jsx)(n.h4,{id:"without-autopilot",children:"Without autopilot"}),"\n",(0,s.jsx)(n.p,{children:"In scenarios involving those when a node joins a Raft cluster, it attempts to\ncatch up with the rest of the nodes through the data that it's replicating from\nthe leader. While in this initial synchronisation state, the node cannot\nvote but is counted for the purposes of quorum. If a number of new nodes join\nthe cluster simultaneously or at similar times, and thereby exceeding the failure\ntolerance of the cluster, quorum may be lost and the cluster can fail."}),"\n",(0,s.jsx)(n.p,{children:"For example, consider a scenario where there is a 3-node cluster with a large\namount of data and a failure tolerance of 1. An additional 3 new nodes then\njoin the cluster. The cluster now consists of 6 nodes with a failure tolerance\nof 2, but since all 3 nodes are still catching up, this will result in a loss of\nquorum."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A 3 node cluster with a large amount of data that's at a failure tolerance of 1."}),"\n",(0,s.jsx)(n.li,{children:"Another 3 new nodes then join the cluster together."}),"\n",(0,s.jsx)(n.li,{children:"Now the cluster consists of 6 nodes with a failure tolerance of 2, but all 3 new nodes are still catching up, resulting in a loss of quorum."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For this reason, we recommend ensuring new nodes have Raft indexes that are\nclose to the leader before adding additional nodes. Raft indexes are visible via\n",(0,s.jsx)(n.code,{children:"bao status"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"deployment-table",children:"Deployment table"}),"\n",(0,s.jsxs)(n.p,{children:["Below is a table that shows quorum size and failure tolerance for various\ncluster sizes. The recommended production deployment consists of 5 servers. A\nsingle server deployment is ",(0,s.jsx)(n.em,{children:(0,s.jsx)(n.strong,{children:"highly"})})," discouraged as data loss is inevitable\nin a failure scenario."]}),"\n",(0,s.jsxs)("table",{class:"table table-bordered table-striped",children:[(0,s.jsx)("thead",{children:(0,s.jsxs)("tr",{children:[(0,s.jsx)("th",{children:"Servers"}),(0,s.jsx)("th",{children:"Quorum Size"}),(0,s.jsx)("th",{children:"Failure Tolerance"})]})}),(0,s.jsxs)("tbody",{children:[(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"1"}),(0,s.jsx)("td",{children:"0"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"2"}),(0,s.jsx)("td",{children:"2"}),(0,s.jsx)("td",{children:"0"})]}),(0,s.jsxs)("tr",{class:"warning",children:[(0,s.jsx)("td",{children:"3"}),(0,s.jsx)("td",{children:"2"}),(0,s.jsx)("td",{children:"1"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"4"}),(0,s.jsx)("td",{children:"3"}),(0,s.jsx)("td",{children:"1"})]}),(0,s.jsxs)("tr",{class:"warning",children:[(0,s.jsx)("td",{children:"5"}),(0,s.jsx)("td",{children:"3"}),(0,s.jsx)("td",{children:"2"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"6"}),(0,s.jsx)("td",{children:"4"}),(0,s.jsx)("td",{children:"2"})]}),(0,s.jsxs)("tr",{children:[(0,s.jsx)("td",{children:"7"}),(0,s.jsx)("td",{children:"4"}),(0,s.jsx)("td",{children:"3"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"minimums--scaling",children:"Minimums & scaling"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"/tutorials/day-one-raft/raft-reference-architecture#recommended-architecture",children:"OpenBao Reference Architecture"}),"\nrecommends a 5 node cluster to ensure a minimum failure tolerance of at least 2."]}),"\n",(0,s.jsx)(n.p,{children:"It is good practise, wherever possible, to retain a failure tolerance of 2 or\nmore."}),"\n",(0,s.jsx)(n.p,{children:"A scaling approach can be pursued in the event of maintenance and other changes\nwhere an additional pair of nodes (ie two) are added in an existing 5 node cluster\nmaking for a 7 node cluster. Once new joiners are confirmed to be in sync then\nthe 2 older nodes can be stopped and or destroyed with the same processes being\nrepeated until all other nodes have been replaced. This use of additional nodes\non a temporary basis of a 7 node cluster arrangement, concluding back to 5 nodes,\nmay be one way to ensure sufficient failure tolerance is maintained and that\nchanges are made progressively in proportion to the cluster failure tolerance and\nnever exceeding the available failure tolerance in any given time."}),"\n",(0,s.jsx)(n.p,{children:"The intent with any change or scaling ought to be with the lose of quorum and\nreduction of the quorum failure tolerances at the forefront and discouraging\nany practises that compromise that."}),"\n",(0,s.jsx)(n.p,{children:"Scaling clusters up or down in pairs with 2 nodes each time also has the added\nadvantage of avoiding even numbers and it is always recommended to\nallow for an odd number of total voters in any cluster."})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>r});var s=t(96540);const a={},i=s.createContext(a);function o(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);